# -*- coding: utf-8 -*-
"""SVM_pos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mzsmFZFWjNCABlLLl8ZUBtgN2rGq9zgv
"""

import nltk
from nltk.corpus import brown
import numpy as np
from sklearn.model_selection import KFold
from gensim.models import Word2Vec
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

nltk.download('brown')
nltk.download('universal_tagset')

dataset_sentances = brown.sents()
dataset_tagged = brown.tagged_sents(tagset = 'universal')
w2v_mapping = Word2Vec(dataset_sentances, size=50)

tags_set = list(set([tag for (_, tag) in brown.tagged_words(tagset='universal')]))
tags_set.extend(["<^>","<$>"])
print(tags_set)
tags_set.sort()
tags={}
for i,tag in enumerate(tags_set):
  tags[tag]=i
tags_list = list(tags_set)

def OneHotEncoder(tag, total_tag):
    return np.eye(total_tag)[tag]

def featureExtractor(word_id, sample, word2vec,tags):
    feature_vector = []
    vocab = word2vec.wv.vocab.keys()
    #Add word2vec embedding of current and previous word
    no_of_prev_words = 1
    for idx in reversed(range(0,no_of_prev_words+1)):
        if sample[word_id-idx][0] not in vocab:
            feature_vector.append(np.zeros(word2vec.vector_size))
        else:
            feature_vector.append(word2vec[sample[word_id-idx][0]])

    #Add word2vec embedding of next word
    no_of_next_words = 1
    for idx in range(1,no_of_next_words+1):
        if sample[word_id+idx][0] not in vocab:
            feature_vector.append(np.zeros(word2vec.vector_size))
        else:
            feature_vector.append(word2vec[sample[word_id+idx][0]])
    #Add one-hot embedding of previous 3 tags
    no_of_prev_tags = 3
    for idx in reversed(range(1,no_of_prev_tags+1)):
        tag = sample[word_id-idx][1]
        feature_vector.append(OneHotEncoder(tags.index(tag),len(tags)))

    #Set the bit if word has verb prefix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][:5].lower() in ["trans"] or sample[word_id][0][:4].lower() in ["over","fore"] or sample[word_id][0][:3].lower() in ["mis","out","pre","sub"] or sample[word_id][0][:2].lower() in ["un","be","de"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if word has verb suffix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][-3:].lower() in ["ise","ate"] or sample[word_id][0][-2:].lower() in ["fy","en"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if word has noun prefix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][:5].lower() in ["hyper","super","ultra"] or sample[word_id][0][:4].lower() in ["anti","semi","auto","kilo","mega","mini","mono","poly"] or sample[word_id][0][:3].lower() in ["mis","out","sub"] or sample[word_id][0][:2].lower() in ["bi","in"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if word has noun suffix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][-4:].lower() in ["tion","sion","ment","ance","ence","ship","ness"] or sample[word_id][0][-3:].lower() in ["ism","ity","ant","ent","age","ery"] or sample[word_id][0][-2:].lower() in ["ry","al","er","cy"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if word has adjective prefix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][:3].lower() in ["non"] or sample[word_id][0][:2].lower() in ["im","in","ir",'il']:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has adjective suffix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][-4:].lower() in ["able","less"] or sample[word_id][0][-3:].lower() in ["ive","ous","ful"] or sample[word_id][0][-2:].lower() in ["al"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if next word has verb prefix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][:5].lower() in ["trans"] or sample[word_id+1][0][:4].lower() in ["over","fore"] or sample[word_id+1][0][:3].lower() in ["mis","out","pre","sub"] or sample[word_id+1][0][:2].lower() in ["un","be","de"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has verb suffix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][-3:].lower() in ["ise","ate"] or sample[word_id+1][0][-2:].lower() in ["fy","en"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if next word has noun prefix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][:5].lower() in ["hyper","super","ultra"] or sample[word_id+1][0][:4].lower() in ["anti","semi","auto","kilo","mega","mini","mono","poly"] or sample[word_id+1][0][:3].lower() in ["mis","out","sub"] or sample[word_id+1][0][:2].lower() in ["bi","in"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has noun suffix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][-4:].lower() in ["tion","sion","ment","ance","ence","ship","ness"] or sample[word_id+1][0][-3:].lower() in ["ism","ity","ant","ent","age","ery"] or sample[word_id+1][0][-2:].lower() in ["ry","al","er","cy"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if next word has adjective prefix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][:3].lower() in ["non"] or sample[word_id+1][0][:2].lower() in ["im","in","ir",'il']:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has adjective suffix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][-4:].lower() in ["able","less"] or sample[word_id+1][0][-3:].lower() in ["ive","ous","ful"] or sample[word_id+1][0][-2:].lower() in ["al"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])


    #Set the bit if it is starting of sample
    if word_id ==0:
      feature_vector.append([1])
    else:
        feature_vector.append([0])

    #Set the bit if it is ending of sample
    if word_id == len(sample)-1:
      feature_vector.append([1])
    else:
        feature_vector.append([0])
      
    #Set the bit if  all letter of word are capital
    if sample[word_id][0].upper() == sample[word_id][0]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])

    #Set the bit if all letter of word are small
    if sample[word_id][0].lower() == sample[word_id][0]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])
      
    #Set the bit if first letter of word is capital   
    if sample[word_id][0][0].isupper():
        feature_vector.append([1])
    else:
        feature_vector.append([0])

    #Set the bit if letters other than 1st letter are capital
    if sample[word_id][0][1:].lower() != sample[word_id][0][1:]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])
      
    #Set the bit if word is numberic
    if sample[word_id][0].isdigit():
      feature_vector.append([1])
    else:
        feature_vector.append([0])
    
    #Set the bit if word contains a "-"
    if '-' in sample[word_id][0]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])

    flat_list = [item for sublist in feature_vector for item in sublist]
    return flat_list

print("Extracting Features")
X_dataset=[]
Y_dataset=[]
for sample in tqdm(dataset_tagged):
    sample = [("","<^>")] + sample + [("","<$>")]
    for j in range(2,len(sample)-1):
        X_dataset.append(featureExtractor(j, sample, w2v_mapping, tags_list))
        Y_dataset.append(tags[sample[j][1]])

class SVM:

    def __init__(self):
        self.W = None 

    def train(self, X, y, batch_size=200, lr=1e-3, regularization = 1e1):
        
        total_tags = np.max(y) + 1

        if self.W is None:
          self.W = np.random.normal(0,1, size=(total_tags, X.shape[0]))

        losses_history = []

        for i in range(0,X.shape[1], batch_size):
            loss, grad = self.calculate_gradient(X[:, i:i+batch_size], y[i:i+batch_size], regularization ) 
            losses_history.append(loss)
            self.W -= lr * grad
        return losses_history

    def calculate_gradient(self, X, y, regularization):
      W=self.W
      dW = np.zeros(W.shape)
      loss = 0.0
      delta = 1.0

      batch_size = y.shape[0]
      y_values = W.dot(X)
      y_values_true = y_values[y, range(batch_size)] 
      
      y_margin = y_values - y_values_true + delta
      y_margin = np.maximum(0, y_margin)
      y_margin[y, range(batch_size)] = 0

      loss = np.sum(y_margin) / batch_size + 0.5 * regularization * np.sum(W * W)

      y_grad = np.zeros(y_values.shape)
      y_incorrect_count = np.sum(y_margin > 0, axis=0)
      y_grad[y_margin > 0] = 1
      y_grad[y, range(batch_size)] = -1 * y_incorrect_count

      dW = y_grad.dot(X.T) / batch_size + regularization * W
      return loss, dW
    
    def predict(self, X):
        
        pred_ys = np.zeros(X.shape[1])
        f_x_mat = self.W.dot(X)
        pred_ys = np.argmax(f_x_mat, axis=0)
        h_x_mat = 1.0 / (1.0 + np.exp(-f_x_mat))
        h_x_mat = h_x_mat.squeeze()
        return pred_ys, h_x_mat

X_dataset=np.array(X_dataset).astype(np.float)
Y_dataset=np.array(Y_dataset)

X_dataset.shape

print("Traing POS-TAGGER")
model = SVM()
epoch=20
crosss_validator = KFold(n_splits=5, random_state=42, shuffle=False)
for i in range(epoch):
  epoch_accuracy=[]
  epoch_loss=[]
  for train_index, test_index in crosss_validator.split(X_dataset):
    X_train, X_test, y_train, y_test = X_dataset[train_index], X_dataset[test_index], Y_dataset[train_index], Y_dataset[test_index]
    X_train=np.transpose(X_train)
    X_test=np.transpose(X_test)
    loss = model.train(X_train, y_train, batch_size=128, lr=0.01,regularization = 0.001)
    epoch_loss.extend(loss)
    y_pred = model.predict(X_test)[0]
    epoch_accuracy.append(np.mean(y_test == y_pred))
  print ('epoch no : %d  accuracy: %f  loss: %f' % (i+1,np.mean(epoch_accuracy),np.mean(epoch_loss)) )

predicted_tag=[]
true_tag=[]
for train_index, test_index in crosss_validator.split(X_dataset):
    X_train, X_test, y_train, y_test = X_dataset[train_index], X_dataset[test_index], Y_dataset[train_index], Y_dataset[test_index]
    X_test=np.transpose(X_test)
    y_pred = model.predict(X_test)[0]
    predicted_tag.extend(y_pred)
    true_tag.extend(y_test)

print("Accuracy is : ",accuracy_score(true_tag, predicted_tag)*100)
print("Classification report")
print(classification_report(true_tag, predicted_tag))
print("Confusion Matrix")
cm_df = pd.DataFrame(confusion_matrix(true_tag, predicted_tag,labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13]),index = ['.', '<$>', '<^>','ADJ','ADP', 'ADV', 'CONJ','DET', 'NOUN', 'NUM','PRON', 'PRT', 'VERB', 'X'], columns =['.', '<$>', '<^>','ADJ','ADP', 'ADV', 'CONJ','DET', 'NOUN', 'NUM','PRON', 'PRT', 'VERB', 'X'])
cm_df.drop(columns=["<$>","<^>"],inplace=True)
cm_df.drop(["<$>","<^>"],inplace=True)
cm_df=cm_df.div(cm_df.sum(axis=1)*0.01, axis=0).fillna(0)
plt.figure(figsize=(8,6))
ax=sns.heatmap(cm_df, annot=True ,fmt=".1f", cmap="Reds")
ax.set_ylim(12, 0)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# print("Per Tag Accuracy")
per_class_accuracy=pd.DataFrame(np.diag(cm_df), index=[cm_df.index])
ax = per_class_accuracy[[0]].plot(kind='bar', title ="Per Tag Accuracy", figsize=(10, 8), fontsize=12,legend=False)
for i, v in enumerate(list(per_class_accuracy[0])):
    plt.text(i - 0.30, v + 0.5, str(v)[:5])
ax.set_xlabel("Tag", fontsize=12)
ax.set_ylabel("Accuracy", fontsize=12)
plt.show()



