# -*- coding: utf-8 -*-
"""SVM_pos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mzsmFZFWjNCABlLLl8ZUBtgN2rGq9zgv
"""

import nltk
from nltk.corpus import brown
import numpy as np
from sklearn.model_selection import KFold
from gensim.models import Word2Vec
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

def OneHotEncoder(tag, total_tag):
    return np.eye(total_tag)[tag]

def featureExtractor(word_id, sample, word2vec,tags):
    feature_vector = []
    vocab = word2vec.wv.vocab.keys()
    #Add word2vec embedding of current and previous word
    no_of_prev_words = 1
    for idx in reversed(range(0,no_of_prev_words+1)):
        if sample[word_id-idx][0] not in vocab:
            feature_vector.append(np.zeros(word2vec.vector_size))
        else:
            feature_vector.append(word2vec[sample[word_id-idx][0]])

    #Add word2vec embedding of next word
    no_of_next_words = 1
    for idx in range(1,no_of_next_words+1):
        if sample[word_id+idx][0] not in vocab:
            feature_vector.append(np.zeros(word2vec.vector_size))
        else:
            feature_vector.append(word2vec[sample[word_id+idx][0]])
    #Add one-hot embedding of previous 3 tags
    no_of_prev_tags = 2
    for idx in reversed(range(1,no_of_prev_tags+1)):
        tag = sample[word_id-idx][1]
        feature_vector.append(OneHotEncoder(tags.index(tag),len(tags)))

    #Set the bit if word has verb prefix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][:5].lower() in ["trans"] or sample[word_id][0][:4].lower() in ["over","fore"] or sample[word_id][0][:3].lower() in ["mis","out","pre","sub"] or sample[word_id][0][:2].lower() in ["un","be","de"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if word has verb suffix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][-3:].lower() in ["ise","ate"] or sample[word_id][0][-2:].lower() in ["fy","en"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if word has noun prefix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][:5].lower() in ["hyper","super","ultra"] or sample[word_id][0][:4].lower() in ["anti","semi","auto","kilo","mega","mini","mono","poly"] or sample[word_id][0][:3].lower() in ["mis","out","sub"] or sample[word_id][0][:2].lower() in ["bi","in"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if word has noun suffix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][-4:].lower() in ["tion","sion","ment","ance","ence","ship","ness"] or sample[word_id][0][-3:].lower() in ["ism","ity","ant","ent","age","ery"] or sample[word_id][0][-2:].lower() in ["ry","al","er","cy"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if word has adjective prefix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][:3].lower() in ["non"] or sample[word_id][0][:2].lower() in ["im","in","ir",'il']:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has adjective suffix
    if len(sample[word_id][0]) > 4:
        if sample[word_id][0][-4:].lower() in ["able","less"] or sample[word_id][0][-3:].lower() in ["ive","ous","ful"] or sample[word_id][0][-2:].lower() in ["al"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if next word has verb prefix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][:5].lower() in ["trans"] or sample[word_id+1][0][:4].lower() in ["over","fore"] or sample[word_id+1][0][:3].lower() in ["mis","out","pre","sub"] or sample[word_id+1][0][:2].lower() in ["un","be","de"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has verb suffix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][-3:].lower() in ["ise","ate"] or sample[word_id+1][0][-2:].lower() in ["fy","en"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if next word has noun prefix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][:5].lower() in ["hyper","super","ultra"] or sample[word_id+1][0][:4].lower() in ["anti","semi","auto","kilo","mega","mini","mono","poly"] or sample[word_id+1][0][:3].lower() in ["mis","out","sub"] or sample[word_id+1][0][:2].lower() in ["bi","in"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has noun suffix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][-4:].lower() in ["tion","sion","ment","ance","ence","ship","ness"] or sample[word_id+1][0][-3:].lower() in ["ism","ity","ant","ent","age","ery"] or sample[word_id+1][0][-2:].lower() in ["ry","al","er","cy"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])

    #Set the bit if next word has adjective prefix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][:3].lower() in ["non"] or sample[word_id+1][0][:2].lower() in ["im","in","ir",'il']:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])
    
    #Set the bit if next word has adjective suffix
    if len(sample[word_id+1][0]) > 4:
        if sample[word_id+1][0][-4:].lower() in ["able","less"] or sample[word_id+1][0][-3:].lower() in ["ive","ous","ful"] or sample[word_id+1][0][-2:].lower() in ["al"]:
            feature_vector.append([1])
        else: 
            feature_vector.append([0])
    else:
        feature_vector.append([0])


    #Set the bit if it is starting of sample
    if word_id ==0:
      feature_vector.append([1])
    else:
        feature_vector.append([0])

    #Set the bit if it is ending of sample
    if word_id == len(sample)-1:
      feature_vector.append([1])
    else:
        feature_vector.append([0])
      
    #Set the bit if  all letter of word are capital
    if sample[word_id][0].upper() == sample[word_id][0]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])

    #Set the bit if all letter of word are small
    if sample[word_id][0].lower() == sample[word_id][0]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])
      
    #Set the bit if first letter of word is capital   
    if sample[word_id][0][0].isupper():
        feature_vector.append([1])
    else:
        feature_vector.append([0])

    #Set the bit if letters other than 1st letter are capital
    if sample[word_id][0][1:].lower() != sample[word_id][0][1:]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])
      
    #Set the bit if word is numeric
    if sample[word_id][0].isdigit():
      feature_vector.append([1])
    else:
        feature_vector.append([0])
    
    #Set the bit if word contains a "-"
    if '-' in sample[word_id][0]:
      feature_vector.append([1])
    else:
        feature_vector.append([0])

    flat_list = [item for sublist in feature_vector for item in sublist]
    return flat_list

class SVM:

    def __init__(self):
        self.W = None 

    def train(self, X, y, batch_size=200, lr=1e-3, regularization = 1e1):
        
        total_tags = np.max(y) + 1

        if self.W is None:
          self.W = np.random.normal(0,1, size=(total_tags, X.shape[0]))

        losses_history = []

        for i in range(0,X.shape[1], batch_size):
            loss, grad = self.calculate_gradient(X[:, i:i+batch_size], y[i:i+batch_size], regularization ) 
            losses_history.append(loss)
            self.W -= lr * grad
        return losses_history

    def calculate_gradient(self, X, y, regularization):
      W=self.W
      dW = np.zeros(W.shape)
      loss = 0.0
      delta = 1.0

      batch_size = y.shape[0]
      y_values = W.dot(X)
      y_values_true = y_values[y, range(batch_size)] 
      
      y_margin = y_values - y_values_true + delta
      y_margin = np.maximum(0, y_margin)
      y_margin[y, range(batch_size)] = 0

      loss = np.sum(y_margin) / batch_size + 0.5 * regularization * np.sum(W * W)

      y_grad = np.zeros(y_values.shape)
      y_incorrect_count = np.sum(y_margin > 0, axis=0)
      y_grad[y_margin > 0] = 1
      y_grad[y, range(batch_size)] = -1 * y_incorrect_count

      dW = y_grad.dot(X.T) / batch_size + regularization * W
      return loss, dW
    
    def predict(self, X):
        return np.argmax(self.W.dot(X), axis=0)

def pos_tagger(text,model):
  sample=[]
  for i in text.split():
    sample.append([i,"X"])
  sample = [["","<^>"],["","<^>"]] + sample + [["","<$>"]]
  for j in range(2,len(sample)-1):
    x=[featureExtractor(j, sample, w2v_mapping, tags_list)]
    inp=np.array(x).astype(np.float).T
    y_pred = model.predict(inp)
    sample[j][1]=tags_list[y_pred[0]]
  return sample[2:-1]

nltk.download('brown')
nltk.download('universal_tagset')
dataset_sentances = brown.sents()
dataset_tagged = brown.tagged_sents(tagset = 'universal')
w2v_mapping = Word2Vec(dataset_sentances, size=50)
print("Universal Tags :")
tags_set = list(set([tag for (_, tag) in brown.tagged_words(tagset='universal')]))
tags_set.extend(["<^>","<$>"])
print(tags_set)
tags_set.sort()
tags={}
for i,tag in enumerate(tags_set):
  tags[tag]=i
tags_list = list(tags_set)
print("Extracting Features")
X_dataset=[]
Y_dataset=[]
for sample in tqdm(dataset_tagged):
    sample = [("","<^>"),("","<^>"),("","<^>")] + sample + [("","<$>")]
    for j in range(3,len(sample)-1):
        X_dataset.append(featureExtractor(j, sample, w2v_mapping, tags_list))
        Y_dataset.append(tags[sample[j][1]])
X_dataset=np.array(X_dataset).astype(np.float)
Y_dataset=np.array(Y_dataset)

print("Traing POS-TAGGER")
model = SVM()
epoch=40
crosss_validator = KFold(n_splits=5, random_state=42, shuffle=False)
for i in range(epoch):
  epoch_accuracy=[]
  epoch_loss=[]
  for train_index, test_index in crosss_validator.split(X_dataset):
    X_train, X_test, y_train, y_test = X_dataset[train_index], X_dataset[test_index], Y_dataset[train_index], Y_dataset[test_index]
    loss = model.train(X_train.T, y_train, batch_size=256, lr=0.01,regularization = 0.0001)
    epoch_loss.extend(loss)
    y_pred = model.predict(X_test.T)
    epoch_accuracy.append(np.mean(y_test == y_pred))
  print ('epoch no : %d  accuracy: %f  loss: %f' % (i+1,np.mean(epoch_accuracy),np.mean(epoch_loss)) )

predicted_tag=[]
true_tag=[]

for train_index, test_index in crosss_validator.split(X_dataset):
    X_train, X_test, y_train, y_test = X_dataset[train_index], X_dataset[test_index], Y_dataset[train_index], Y_dataset[test_index]
    X_test=np.transpose(X_test)
    y_pred = model.predict(X_test)
    predicted_tag.extend(y_pred)
    true_tag.extend(y_test)


print("Accuracy is : ",accuracy_score(true_tag, predicted_tag)*100)
print("Classification report")
print(classification_report(true_tag, predicted_tag))
print("Confusion Matrix")
cm_df = pd.DataFrame(confusion_matrix(true_tag, predicted_tag,labels=range(14)),index = tags_list, columns =tags_list)
cm_df.drop(columns=["<$>","<^>"],inplace=True)
cm_df.drop(["<$>","<^>"],inplace=True)
cm_df=cm_df.div(cm_df.sum(axis=1)*0.01, axis=0).fillna(0)
plt.figure(figsize=(10,8))
ax=sns.heatmap(cm_df, annot=True ,fmt=".2f", cmap="Reds")
ax.set_ylim(12, 0)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# %matplotlib inline
plt.figure(figsize=(35,8))
ax=sns.heatmap(np.absolute(model.W), annot=False , cmap="Reds",yticklabels=tags_list)
plt.title('SVM Weights')
ax.set_ylim(13, 0)
plt.ylabel('Tags')
plt.xlabel('Features')
plt.show()

# hmm_per_class=[99.977626,87.330441,97.196992, 86.039065,99.281085,98.716235,90.614809,76.914278, 95.758399,83.165028,90.286143,18.84058 ]
# bilstm_per_class=[99.92, 94.86, 98.69, 97.59, 99.59, 99.65, 98.86, 97.74, 99.49, 97.67,99.58 , 83.65]

# per_class_accuracy=pd.DataFrame(np.diag(cm_df), index=[cm_df.index])
# per_class_accuracy["SVM"]=per_class_accuracy[0]
# per_class_accuracy["HMM"]=hmm_per_class
# per_class_accuracy["BiLSTM"]=bilstm_per_class
# ax = per_class_accuracy[['SVM','HMM',"BiLSTM"]].plot(kind='bar', title ="Per Tag Accuracy Comparision", figsize=(25, 8), fontsize=14,legend=True)
# for i, v in enumerate(list(per_class_accuracy['SVM'])):
#     plt.text(i - 0.25, v + 0.5, str(v)[:4], fontsize=8)
# for i, v in enumerate(list(per_class_accuracy['HMM'])):
#     plt.text(i - 0.08, v + 0.5, str(v)[:4], fontsize=8)
# for i, v in enumerate(list(per_class_accuracy['BiLSTM'])):
#     plt.text(i + 0.10, v + 0.5, str(v)[:4], fontsize=8)
# # aset_color('r')
# ax.set_xlabel("Tag", fontsize=12)
# ax.set_ylabel("Accuracy (%)", fontsize=12)
# # ax.tight_layout()
# plt.show()



