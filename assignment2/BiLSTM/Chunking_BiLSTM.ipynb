{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Chunking::BiLSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYD0NKkGWllW"
      },
      "source": [
        "Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFpf8Y6vSyeZ",
        "outputId": "5fa798a0-d1bc-4eab-f5df-514aa307fb43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1cwa5pYljOG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from gensim.models import Word2Vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JbD-owbhqO9",
        "outputId": "8e70bde6-880b-41f1-d239-d1cb3258edc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Glove dictionary 300 dimentions\n",
        "!wget https://www.cse.iitb.ac.in/~kartavya/glove_dict_tensor.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-29 09:42:30--  https://www.cse.iitb.ac.in/~kartavya/glove_dict_tensor.pkl\n",
            "Resolving www.cse.iitb.ac.in (www.cse.iitb.ac.in)... 103.21.127.134\n",
            "Connecting to www.cse.iitb.ac.in (www.cse.iitb.ac.in)|103.21.127.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 770589404 (735M)\n",
            "Saving to: ‘glove_dict_tensor.pkl.3’\n",
            "\n",
            "glove_dict_tensor.p 100%[===================>] 734.89M  5.29MB/s    in 2m 28s  \n",
            "\n",
            "2020-10-29 09:45:00 (4.95 MB/s) - ‘glove_dict_tensor.pkl.3’ saved [770589404/770589404]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWEsaiPjUky2"
      },
      "source": [
        "Load the glove dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8_4dyfsSgD-"
      },
      "source": [
        "import pickle\n",
        "pickle_in = open(\"glove_dict_tensor.pkl\",\"rb\")\n",
        "glove = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by03TOSAqmmt"
      },
      "source": [
        "!wget -q https://www.cse.iitb.ac.in/~sourabj/dataset2/train.txt\n",
        "!wget -q https://www.cse.iitb.ac.in/~sourabj/dataset2/test.txt\n",
        "\n",
        "def load_and_process(filepath):\n",
        "    df = np.loadtxt(filepath, dtype=str)\n",
        "    df[:, -1] = np.vectorize(lambda x: x.split('-')[0])(df[:,-1])\n",
        "    cutfrom = 0\n",
        "    sentences = []\n",
        "    for i in range(df.shape[0]):\n",
        "        # print(ts[i])\n",
        "        if df[i][0] == '.':\n",
        "            sentences.append(df[cutfrom:i+1].T)\n",
        "            cutfrom = i + 1\n",
        "    return sentences\n",
        "\n",
        "\n",
        "testset = load_and_process(\"test.txt\")\n",
        "trainset = load_and_process(\"train.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aawQJOJ70b8P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9rSdWXKvBRW"
      },
      "source": [
        "all_pos = set()\n",
        "for i in testset:\n",
        "    all_pos = all_pos.union(set(i[1]))\n",
        "for i in trainset:\n",
        "    all_pos = all_pos.union(set(i[1]))\n",
        "all_pos = list(all_pos)\n",
        "id2pos = dict(enumerate(all_pos))\n",
        "pos2id = id2tag = {v: k for k, v in id2pos.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPdhctxt4sDU",
        "outputId": "9ca09090-2b7a-4d21-df6d-01aae39f7836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tag2id = { \n",
        "    'B': 0,\n",
        "    'I': 1,\n",
        "    'O': 2,\n",
        "    'PAD': 3\n",
        "}\n",
        "id2tag = {v: k for k, v in tag2id.items()}\n",
        "id2tag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'B', 1: 'I', 2: 'O', 3: 'PAD'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKYDDROuLPCl"
      },
      "source": [
        "def get_sent_and_tag(trainset):\n",
        "    sentences = []\n",
        "    tags = []\n",
        "    for i, data in enumerate(trainset):\n",
        "        # print(i, data[0])\n",
        "        sent = []\n",
        "        for j, word in enumerate(data[0]):\n",
        "            word_embed = glove.get(word.lower(), torch.zeros(200))\n",
        "            posid = 0 #pos2id[data[1][j]]\n",
        "            posid = torch.tensor([posid])\n",
        "            word_embed = (torch.cat((word_embed, posid), 0))\n",
        "            sent.append(word_embed)\n",
        "            # sent[-1].append(0)\n",
        "        # sent = ([glove.get(word.lower(), torch.zeros(200)) for word in data[0]])\n",
        "        sent = torch.cat(sent).view(len(sent),-1)\n",
        "        if len(sent) < 100:\n",
        "            sent = torch.cat((sent,torch.zeros((100-len(sent),201))))\n",
        "\n",
        "        sentences.append(sent[:100])\n",
        "        sent_tag = data[2][:100].tolist()\n",
        "        sent_tag.extend(['PAD']*(100-data.shape[1]))\n",
        "\n",
        "        sent_tag = ([tag2id[x] for x in sent_tag])\n",
        "        sent_tag = torch.tensor(sent_tag)\n",
        "        tags.append(sent_tag)\n",
        "    sentences = torch.stack(sentences)\n",
        "    tags = torch.stack(tags)\n",
        "    return sentences, tags\n",
        "Xtrain, Ytrain = get_sent_and_tag(trainset)\n",
        "Xtest, Ytest = get_sent_and_tag(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_LynUcQipj",
        "outputId": "9e593544-7738-4199-9cb3-f3625bf5f5f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8725, 100, 201]),\n",
              " torch.Size([8725, 100]),\n",
              " torch.Size([1952, 100, 201]),\n",
              " torch.Size([1952, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyUuCY_zquaD"
      },
      "source": [
        "# postags = list(np.unique(trainset[:,1]))\n",
        "# z = np.zeros([trainset.shape[0], len(postags)])\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# testset[:,1] = OneHotEncoder().fit_transform(trainset[:,1].reshape((-1,1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SqM0HGaSQ6B"
      },
      "source": [
        "class NET(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layer_dim, batchSize, seqLength, numClasses):\n",
        "    super(NET,self).__init__()\n",
        "    self.batchSize = batchSize\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.layer_dim = layer_dim\n",
        "\n",
        "    self.bilstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, bidirectional = True)\n",
        "    self.fc = nn.Linear(hidden_dim*2,numClasses)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    # Initialize hidden state\n",
        "    # h0 = torch.zeros((self.layer_dim*2, input_ids.size(0), self.hidden_dim)).requires_grad_()\n",
        "    # # Initialize cell state\n",
        "    # c0 = torch.zeros((self.layer_dim*2, input_ids.size(0), self.hidden_dim)).requires_grad_()\n",
        "    # BiLSTM\n",
        "    # print(input_ids.size())\n",
        "    out , (hn,cn) = self.bilstm(input_ids)\n",
        "    # print('^^^^^^^^^^^',out.size(0))\n",
        "    out = self.fc(out)\n",
        "    # out = F.softmax(out,dim=1)\n",
        "\n",
        "    return out\n",
        "\n",
        "def train(model,data,optimizer,loss_criterion, batch_size):\n",
        "  model.train()\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  \n",
        "  for sentence_batch,tags_batch in data:\n",
        "    if tags_batch.shape[0]!=batch_size:\n",
        "      # print(\"label mismatch\")\n",
        "      continue\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      sentence_batch = sentence_batch.cuda()\n",
        "      tags_batch = tags_batch.cuda()\n",
        "      # label = label.cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(sentence_batch)\n",
        "    out = out.view(-1,out.shape[-1])\n",
        "    tags_batch = tags_batch.view(-1)\n",
        "\n",
        "    loss=loss_criterion(out, tags_batch)\n",
        "    acc = categorical_accuracy(out, tags_batch, tag2id['PAD'])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "  \n",
        "  return epoch_loss / len(data), epoch_acc / len(data)\n",
        "\n",
        "def evaluate(model,data,loss_criterion,batch_size):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sentencAnalysise_batch,tags_batch in data:\n",
        "      if tags_batch.shape[0]!=batch_size:\n",
        "        # print(\"label mismatch\")\n",
        "        continue \n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "        sentence_batch = sentence_batch.cuda()\n",
        "        tags_batch = tags_batch.cuda()\n",
        "\n",
        "      out = model(sentence_batch)\n",
        "      out = out.view(-1,out.shape[-1])\n",
        "      tags_batch = tags_batch.view(-1)\n",
        "\n",
        "      loss=loss_criterion(out, tags_batch)\n",
        "      acc = categorical_accuracy(out, tags_batch, tag2id['PAD'])\n",
        "\n",
        "      # loss.backward()\n",
        "      # optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(data), epoch_acc / len(data)\n",
        "class NET(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layer_dim, batchSize, seqLength, numClasses):\n",
        "    super(NET,self).__init__()\n",
        "    self.batchSize = batchSize\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.layer_dim = layer_dim\n",
        "\n",
        "    self.bilstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, bidirectional = True)\n",
        "    self.fc = nn.Linear(hidden_dim*2,numClasses)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    # Initialize hidden state\n",
        "    # h0 = torch.zeros((self.layer_dim*2, input_ids.size(0), self.hidden_dim)).requires_grad_()\n",
        "    # # Initialize cell state\n",
        "    # c0 = torch.zeros((self.layer_dim*2, input_ids.size(0), self.hidden_dim)).requires_grad_()\n",
        "    # BiLSTM\n",
        "    # print(input_ids.size())\n",
        "    out , (hn,cn) = self.bilstm(input_ids)\n",
        "    # print('^^^^^^^^^^^',out.size(0))\n",
        "    out = self.fc(out)\n",
        "    # out = F.softmax(out,dim=1)\n",
        "\n",
        "    return out\n",
        "\n",
        "def train(model,data,optimizer,loss_criterion, batch_size):\n",
        "  model.train()\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  \n",
        "  for sentence_batch,tags_batch in data:\n",
        "    if tags_batch.shape[0]!=batch_size:\n",
        "      # print(\"label mismatch\")\n",
        "      continue\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      sentence_batch = sentence_batch.cuda()\n",
        "      tags_batch = tags_batch.cuda()\n",
        "      # label = label.cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(sentence_batch)\n",
        "    out = out.view(-1,out.shape[-1])\n",
        "    tags_batch = tags_batch.view(-1)\n",
        "\n",
        "    loss=loss_criterion(out, tags_batch)\n",
        "    acc = categorical_accuracy(out, tags_batch, tag2id['PAD'])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "  \n",
        "  return epoch_loss / len(data), epoch_acc / len(data)\n",
        "\n",
        "def evaluate(model,data,loss_criterion,batch_size):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for sentence_batch,tags_batch in data:\n",
        "      if tags_batch.shape[0]!=batch_size:\n",
        "        # print(\"label mismatch\")\n",
        "        continue \n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "        sentence_batch = sentence_batch.cuda()\n",
        "        tags_batch = tags_batch.cuda()\n",
        "\n",
        "      out = model(sentence_batch)\n",
        "      out = out.view(-1,out.shape[-1])\n",
        "      tags_batch = tags_batch.view(-1)\n",
        "\n",
        "      loss=loss_criterion(out, tags_batch)\n",
        "      acc = categorical_accuracy(out, tags_batch, tag2id['PAD'])\n",
        "\n",
        "      # loss.backward()\n",
        "      # optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(data), epoch_acc / len(data)\n",
        "\n",
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "  \n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True).squeeze(1) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    return accuracy_score(max_preds[non_pad_elements].cpu(),y[non_pad_elements].cpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kkFmW8DSqF3",
        "outputId": "6f24c1c8-ac9b-4519-ba85-1ef6a27cfe8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "\n",
        "\n",
        "input_dim = 201\n",
        "hidden_dim = 50\n",
        "layer_dim = 2\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# import random\n",
        "\n",
        "model=NET(input_dim, hidden_dim, layer_dim,32,40,13)\n",
        "model.double()\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_criterion = nn.CrossEntropyLoss(ignore_index=tag2id['PAD'])\n",
        "\n",
        "for i in tqdm(range(num_epochs)):\n",
        "  epoch_accuracy=[]\n",
        "  epoch_loss=[]\n",
        "\n",
        "  for i in range(1):\n",
        "    # X_train, X_test, y_train, y_test = sentences[train_index], sentences[test_index], sentence_tags[train_index], sentence_tags[test_index]\n",
        "    train_dataset = TensorDataset(Xtrain, Ytrain)\n",
        "    test_dataset = TensorDataset(Xtest, Ytest)\n",
        "    X_train, X_test, y_train, y_test = None,None,None,None\n",
        "    train_index, test_index = None,None\n",
        "    \n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,  # The training samples.\n",
        "        # sampler = RandomSampler(train_dataset), # Select batches randomly #TODO RandomSampler\n",
        "        batch_size = batch_size, # Trains with this batch size.\n",
        "    )\n",
        "    train_dataset = None\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        # sampler = SequentialSampler(test_dataset),\n",
        "        batch_size = batch_size\n",
        "    )\n",
        "    test_dataset = None\n",
        "\n",
        "    # if cuda.is_available():\n",
        "\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, loss_criterion, batch_size)\n",
        "    train_dataloader = None\n",
        "    test_loss, test_acc = evaluate(model, test_dataloader, loss_criterion, batch_size)\n",
        "    test_dataloader = None\n",
        "\n",
        "    epoch_accuracy.append(test_acc)\n",
        "    epoch_loss.append(test_loss)\n",
        "\n",
        "  print ('Accuracy: %f  loss: %f' % (np.mean(epoch_accuracy)*100,np.mean(epoch_loss)) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:17<11:36, 77.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 88.255266  loss: 0.313616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:34<10:19, 77.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 91.721040  loss: 0.229831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [03:52<09:02, 77.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 92.801617  loss: 0.202078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [05:10<07:45, 77.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 93.531929  loss: 0.183862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [06:28<06:28, 77.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 94.095743  loss: 0.170557\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [07:46<05:11, 77.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 94.534543  loss: 0.161797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [09:05<03:54, 78.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 94.674581  loss: 0.156936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [10:23<02:36, 78.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 94.806968  loss: 0.155073\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [11:43<01:18, 78.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 94.884084  loss: 0.155561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [13:02<00:00, 78.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 94.882098  loss: 0.157165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7amj_nLGYtP",
        "outputId": "33ec5c2f-9827-4e1f-c15c-a2523f2bd1b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "def TestEvaluation(model, testSet):\n",
        "  model.eval()\n",
        "  pred_flat = []\n",
        "  labels_flat = [] \n",
        "  \n",
        "  for batch in testSet:\n",
        "    # print(batch)\n",
        "    # b_input_ids = batch[0]\n",
        "    # # b_input_mask = batch[1].to(device)\n",
        "    # b_labels = batch[1]\n",
        "    # if b_labels.shape[0]!=batch_size:\n",
        "    #   continue\n",
        "    labels = batch[2]\n",
        "\n",
        "    with torch.no_grad():\n",
        "    #   out = model(b_input_ids)\n",
        "      pred = predict(batch, model)\n",
        "    # preds = out.detach().numpy()\n",
        "    # pred_flat.extend(np.argmax(preds, axis=1).flatten().tolist())\n",
        "    # labels_flat.extend(b_labels.cpu().numpy().flatten().tolist())\n",
        "    labels_flat.extend(batch[2].tolist())\n",
        "    pred_flat.extend(pred)\n",
        "\n",
        "  print(\"Test Accuracy:\", accuracy_score(labels_flat, pred_flat))\n",
        "  print(classification_report(labels_flat, pred_flat, digits=3))\n",
        "  return labels_flat, pred_flat\n",
        "l, p = TestEvaluation(model, testset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9482962462525862\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.959     0.956     0.958     23846\n",
            "           I      0.929     0.943     0.936     17340\n",
            "           O      0.962     0.931     0.946      6180\n",
            "\n",
            "    accuracy                          0.948     47366\n",
            "   macro avg      0.950     0.944     0.947     47366\n",
            "weighted avg      0.948     0.948     0.948     47366\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-I0QR8fFVAl",
        "outputId": "5116c91b-6869-40c7-d346-0273784fe494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "confusion=np.zeros((3,3),)\n",
        "for i,j in zip(l, p):\n",
        "    confusion[tag2id[i]][tag2id[j]] += 1\n",
        "conf = (confusion.T/(confusion.sum(axis=1))).T\n",
        "conf = np.nan_to_num(conf)\n",
        "plt.figure(figsize=(10,8))\n",
        "ax = sns.heatmap(conf, annot=True, fmt=\".2f\",\n",
        "                 cmap=\"Reds\",\n",
        "                 xticklabels=['B','I','O'],\n",
        "                 yticklabels=['B','I','O'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmtEYr2TqzL4"
      },
      "source": [
        "def predict(batch,model):\n",
        "  sentence = []\n",
        "  tags = []\n",
        "  for word, tag in zip(batch[0], batch[1]):\n",
        "    word_embed = glove.get(word.lower(), torch.zeros(200))\n",
        "    posid = 0#pos2id[tag]\n",
        "    posid = torch.tensor([posid])\n",
        "    word_embed = (torch.cat((word_embed, posid), 0))\n",
        "    sentence.append(word_embed)\n",
        "  sentence = torch.cat(sentence).view(len(sentence),-1)\n",
        "  sentence = torch.stack([sentence])\n",
        "  ds = TensorDataset(sentence)\n",
        "  dl = DataLoader(ds,1)\n",
        "\n",
        "  for i in dl:\n",
        "    # print(i[0])\n",
        "    out = model(i[0])\n",
        "\n",
        "  # print(F.softmax(out[0]).argmax())\n",
        "\n",
        "  return list(map(lambda x: id2tag[x],F.softmax(out[0]).argmax(dim=1).numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ1R6UP89twa"
      },
      "source": [
        "# Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4c85DlTia3-",
        "outputId": "59b39485-d203-4677-cd9e-a4b360c360e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        }
      },
      "source": [
        "i=0\n",
        "incorrect, total = 0,0\n",
        "for t in testset:\n",
        "    pg = (list(zip(predict(t[0], model), t[2])))\n",
        "    if sum([x!=y for x, y in pg]) > len(pg)//3:\n",
        "        # print(t, pg)\n",
        "        printable = ((list(zip(t[0], t[2], predict(t[0], model)))))\n",
        "        for p in printable:\n",
        "            print(f\"<{p[0]}, {p[1]}, {p[2]}>,\", end=\" \")\n",
        "        print()\n",
        "        # print(\"\\t\".join(t[0]))\n",
        "        # print(\"\\t\".join(t[2]))\n",
        "        # print(\"\\t\".join(predict(t[0], model)))\n",
        "    if incorrect > 1000:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<SHEARSON, B, O>, <LEHMAN, I, O>, <HUTTON, I, B>, <Inc, I, I>, <., O, O>, \n",
            "<Advanced, B, O>, <Cardiovascular, I, O>, <Systems, I, O>, <Inc., I, O>, <and, O, O>, <Cardiac, B, I>, <Pacemakers, I, O>, <Inc., I, I>, <units, I, B>, <led, B, B>, <growth, B, B>, <in, B, B>, <the, B, B>, <medical-instrument, I, I>, <systems, I, I>, <division, I, I>, <., O, O>, \n",
            "<'', O, O>, <Rival, B, O>, <Boston, I, O>, <Herald, I, O>, <columnist, I, I>, <Howie, I, O>, <Carr, I, B>, <,, O, O>, <who, B, B>, <usually, B, B>, <rails, B, I>, <at, B, B>, <Statehouse, B, B>, <``, I, O>, <hacks, I, B>, <'', I, O>, <and, I, O>, <nepotism, I, B>, <,, O, O>, <argued, B, B>, <that, B, B>, <the, B, B>, <new, I, I>, <drawings, I, I>, <were, B, B>, <designed, I, I>, <to, B, B>, <hide, I, I>, <Mr., B, B>, <Madden, I, O>, <'s, B, B>, <``, O, O>, <rapidly, B, I>, <growing, I, I>, <forehead, B, B>, <'', O, O>, <and, O, O>, <the, B, B>, <facial, I, I>, <defects, I, I>, <of, B, B>, <``, B, O>, <chinless, I, B>, <'', I, O>, <Dan, I, O>, <Shaughnessy, I, O>, <,, O, O>, <a, B, B>, <Globe, I, I>, <sports, I, I>, <columnist, I, I>, <., O, O>, \n",
            "<Jocelyn, B, O>, <Tomkin, I, O>, <Astronomy, B, O>, <Department, I, O>, <University, B, O>, <of, B, B>, <The, B, B>, <Internal, I, O>, <Revenue, I, O>, <Service, I, O>, <plans, B, B>, <to, I, I>, <restructure, I, I>, <itself, B, B>, <more, B, B>, <like, B, B>, <a, B, B>, <private, I, I>, <corporation, I, I>, <., O, O>, \n",
            "<Mr., B, O>, <Windsor, I, O>, <notified, B, B>, <Everett, B, O>, <E., I, O>, <Briggs, I, O>, <,, O, O>, <the, B, B>, <U.S., I, I>, <ambassador, I, B>, <to, B, B>, <Panama, B, B>, <,, O, O>, <of, B, B>, <the, B, B>, <invitation, I, I>, <., O, O>, \n",
            "<White, B, O>, <Males, I, O>, <White, B, O>, <Females, I, O>, <Black, B, O>, <Males, I, O>, <Black, B, O>, <Females, I, O>, <Directors, B, O>, <elected, B, I>, <R., B, O>, <Marvin, I, B>, <Womack, I, O>, <,, O, O>, <currently, B, B>, <vice, B, B>, <president\\/product, I, I>, <supply, I, I>, <,, I, O>, <purchasing, I, B>, <,, O, O>, <to, B, B>, <head, I, I>, <the, B, B>, <company, I, I>, <'s, B, B>, <Washington, I, I>, <,, I, I>, <D.C., I, I>, <,, I, I>, <office, I, I>, <., O, O>, \n",
            "<Harry, B, O>, <Lee, I, O>, <Smith, I, B>, <Alpharetta, B, B>, <,, O, O>, <Ga, B, B>, <., O, O>, \n",
            "<This, B, O>, <is, B, B>, <written, I, I>, <to, B, B>, <correct, I, I>, <a, B, B>, <misquotation, I, I>, <in, B, B>, <your, B, B>, <Oct., I, I>, <3, I, I>, <article, I, I>, <``, O, O>, <Deaths, B, O>, <From, B, O>, <Advanced, B, O>, <Colon, I, O>, <Cancer, I, O>, <Can, B, O>, <Be, I, O>, <Reduced, I, O>, <by, B, B>, <Using, B, B>, <Two, B, I>, <Drugs, I, I>, <., O, O>, \n",
            "<Charles, B, O>, <G., I, O>, <Moertel, I, B>, <M.D, I, I>, <., O, O>, \n",
            "<'', O, O>, <Senate, B, O>, <Banking, I, O>, <Committee, I, O>, <Chairman, I, O>, <Donald, I, O>, <Riegle, I, O>, <-LRB-, O, O>, <D., B, B>, <,, O, O>, <Mich, B, B>, <., O, O>, \n",
            "<IRS, B, O>, <Notice, I, O>, <89-136, I, O>, <describes, B, B>, <this, B, B>, <and, O, O>, <other, B, B>, <deadline, I, B>, <relief, I, B>, <for, B, B>, <Hugo, B, B>, <'s, B, B>, <victims, I, I>, <., O, O>, \n",
            "<IRS, B, O>, <Revenue, I, O>, <Procedure, I, O>, <89-52, I, O>, <describes, B, B>, <the, B, B>, <reporting, I, I>, <requirements, I, I>, <., O, O>, \n",
            "<Mr., B, B>, <Langton, I, O>, <'s, B, B>, <group, I, I>, <,, O, O>, <Concerned, B, B>, <Off-Road, I, O>, <Bicyclists, I, O>, <Association, I, O>, <,, O, O>, <mounted, B, B>, <petition, B, I>, <drives, I, B>, <to, B, B>, <help, I, I>, <keep, I, I>, <open, B, B>, <certain, B, B>, <Santa, I, I>, <Monica, I, O>, <Mountain, I, O>, <trails, I, B>, <designated, B, B>, <for, B, B>, <closing, B, B>, <., O, O>, \n",
            "<buy-out, I, B>, <., O, O>, \n",
            "<Spaced, B, O>, <Out, I, O>, <Those, B, I>, <supermarket, I, I>, <tabloids, I, I>, <Make, B, B>, <me, B, B>, <feel, B, B>, <slow, B, I>, <Because, B, I>, <I, B, B>, <still, B, B>, <have, B, B>, <n't, I, I>, <seen, I, I>, <--, B, O>, <Bruce, I, B>, <Kafaroff, I, I>, <., O, O>, \n",
            "<--, B, O>, <Daisy, I, B>, <Brown, I, I>, <., O, O>, \n",
            "<The, B, B>, <two, I, B>, <Remics, I, I>, <priced, B, B>, <were, B, B>, <a, B, I>, <$, I, I>, <500, I, I>, <million, I, I>, <Federal, I, O>, <Home, I, O>, <Loan, I, O>, <Mortgage, I, O>, <Corp., I, O>, <issue, I, I>, <underwritten, B, B>, <by, B, B>, <Salomon, B, B>, <Brothers, I, O>, <Inc., I, O>, <and, O, O>, <a, B, B>, <$, I, I>, <350, I, I>, <million, I, I>, <Federal, I, O>, <National, I, O>, <Mortgage, I, O>, <Association, I, O>, <deal, I, B>, <underwritten, B, B>, <by, B, B>, <Greenwich, B, B>, <Capital, I, I>, <Markets, I, I>, <., O, O>, \n",
            "<A, B, O>, <Chemical, I, O>, <Securities, I, I>, <group, I, I>, <won, B, B>, <a, O, B>, <$, O, I>, <100, O, I>, <million, O, I>, <Oregon, O, O>, <general, O, I>, <obligation, O, I>, <veterans, B, I>, <', B, B>, <tax, I, I>, <note, I, I>, <issue, I, B>, <due, B, B>, <Nov., B, B>, <1, I, I>, <,, I, I>, <1990, I, I>, <., O, O>, \n",
            "<-RRB-, O, B>, <., O, O>, \n",
            "<The, B, O>, <House, I, O>, <Energy, I, O>, <Committee, I, O>, <will, B, B>, <debate, I, I>, <the, B, B>, <issue, I, I>, <later, B, B>, <this, I, B>, <month, I, I>, <., O, O>, \n",
            "<Rep., B, O>, <Bill, I, O>, <McCollum, I, O>, <-LRB-, O, O>, <R., B, O>, <,, O, O>, <Fla., B, B>, <-RRB-, O, I>, <reports, B, B>, <that, B, B>, <these, B, B>, <included, B, I>, <20,000, B, B>, <to, I, B>, <30,000, I, B>, <Soviet, I, I>, <Central, I, O>, <Asian, I, O>, <KGB, I, O>, <Border, I, O>, <Guards, I, B>, <,, O, O>, <ethnically, B, B>, <indistinguishable, I, B>, <from, B, B>, <Afghans, B, B>, <and, O, O>, <wearing, B, B>, <unmarked, B, B>, <uniforms, I, B>, <., O, O>, \n",
            "<All, B, O>, <this, I, B>, <causes, B, I>, <Rep., B, B>, <Hyde, I, O>, <to, B, B>, <muse, I, B>, <about, B, B>, <an, B, B>, <alternate, I, I>, <way, I, B>, <to, B, B>, <drum, I, I>, <up, B, B>, <more, B, B>, <enthusiasm, I, I>, <., O, O>, \n",
            "<Judge, B, O>, <Masaaki, I, O>, <Yoneyama, I, O>, <told, B, B>, <the, B, B>, <Osaka, I, I>, <District, I, O>, <Court, I, O>, <Daikin, B, O>, <'s, B, B>, <``, I, I>, <responsibility, I, I>, <is, B, B>, <heavy, B, I>, <because, B, I>, <illegal, B, B>, <exports, I, I>, <lowered, B, B>, <international, B, B>, <trust, I, I>, <in, B, B>, <Japan, B, B>, <., O, O>, \n",
            "<New, B, O>, <York, I, O>, <Stock, I, O>, <Exchange, I, O>, <volume, I, I>, <was, B, B>, <a, B, B>, <heavy, I, I>, <224,070,000, I, I>, <shares, I, I>, <., O, O>, \n",
            "<UAL, B, O>, <was, B, B>, <watched, I, I>, <closely, B, B>, <and, O, I>, <traded, B, I>, <heavily, B, B>, <., O, O>, \n",
            "<Both, B, B>, <Citicorp, B, I>, <and, O, I>, <Manufacturers, B, I>, <Hanover, I, I>, <reported, B, B>, <earnings, B, B>, <yesterday, B, I>, <., O, O>, \n",
            "<The, B, O>, <Dow, I, O>, <Jones, I, O>, <Transportation, I, O>, <Average, I, B>, <fell, B, B>, <49.96, B, B>, <to, B, B>, <close, I, I>, <at, B, B>, <1254.27, B, B>, <., O, O>, \n",
            "<The, B, O>, <Amex, I, O>, <Market, I, O>, <Value, I, O>, <Index, I, O>, <fell, B, B>, <1.25, B, B>, <to, B, B>, <375.16, B, B>, <., O, O>, \n",
            "<Carnival, B, O>, <Cruise, I, O>, <Lines, I, O>, <Class, I, O>, <A, I, O>, <dropped, B, B>, <1, B, B>, <to, B, B>, <21, B, B>, <1\\/8, I, I>, <on, B, B>, <331,400, B, B>, <shares, I, I>, <., O, O>, \n",
            "<The, B, O>, <G-7, I, O>, <comprises, B, B>, <West, B, B>, <Germany, I, I>, <,, O, O>, <the, B, B>, <U.S., I, I>, <,, O, I>, <France, B, I>, <,, O, O>, <the, B, B>, <U.K., I, I>, <,, O, I>, <Italy, B, I>, <,, O, I>, <Canada, B, I>, <and, O, I>, <Japan, B, I>, <., O, O>, \n",
            "<P&G, B, B>, <already, B, B>, <sells, B, B>, <its, B, B>, <Folgers, I, I>, <ground, I, I>, <roast, I, I>, <coffee, I, B>, <to, B, B>, <food, B, I>, <service, I, B>, <concerns, I, I>, <,, B, O>, <but, I, B>, <not, I, I>, <to, B, I>, <as, B, I>, <many, I, B>, <markets, I, I>, <as, B, B>, <Maryland, B, B>, <Club, I, I>, <., O, O>, \n",
            "<Rep., B, O>, <Lee, I, O>, <Hamilton, I, O>, <-LRB-, O, B>, <D., B, B>, <,, O, O>, <Ind, B, B>, <., O, O>, \n",
            "<Both, B, B>, <Shearson, B, O>, <'s, B, B>, <Mr, I, I>, <., I, O>, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L01ScbH69CYa",
        "outputId": "eaef8ce1-37a1-491a-e682-cd64051f34e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "testset[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Rockwell', 'International', 'Corp.', \"'s\", 'Tulsa', 'unit',\n",
              "       'said', 'it', 'signed', 'a', 'tentative', 'agreement', 'extending',\n",
              "       'its', 'contract', 'with', 'Boeing', 'Co.', 'to', 'provide',\n",
              "       'structural', 'parts', 'for', 'Boeing', \"'s\", '747', 'jetliners',\n",
              "       '.'], dtype='<U54')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    }
  ]
}